---
aliases:
- policy
tags:
- ml
---
# policy
> [!info] Intro: 
> In machine learning we call  policy ($\pi$) to the set of rules that define which actions to take and with what probabilty to take them given some situation. 

We can define some policy ($\pi$) for some situation|time (t) and some action (a) as:
$$
\pi_{t}(a) =  \frac{e^{H_{t}(a)}}{\sum_{b=1}^ke^{H_{t}(b)}} = Pr\{A_{t}=a\}
$$
**where:**
 - we'll just say: **The probability (Pr) of taking action a at time t**
 - $H_{t}(a)$ -> Defines the **preference** for action a
 - The sum downstaris sums over the preferences over all actions. 
 - The next interesting step to look at is how to [[#calculating H|get H(a)]]
- Initially all preferences are set to 0. All actions have equal probability. 
## calculating H

We'll use the obtained rewards and previous policies:
Given some taken action $A_{t}$. We update the preference for that action and then update every other preference for every other action a. 
$$
\begin{align} \\
H_{t+1}(A_{t}) =& H_{t}(A_{t})+ \alpha(R_{t}-\overline R_{t})(1-\pi_{t}(A_{t}))\\ \\
H_{t+1}(a) =& H_{t}(a)- \alpha(R_{t}-\overline R_{t})\pi_{t}(a)\\ \\ \\
\end{align}
$$
This adjustment makes it so that the complete policy still maintains a total probability of 1 for the sum of all actions. 

## types of policies:
Some of the most used policies are:
- [[1762091835-greedypolicy|greedy policy]]

***
### Up
- [[1762025473-reinforcementlearning|reinforcement learning]]
### Down
- [[1762091835-greedypolicy|greedy policy]]
***