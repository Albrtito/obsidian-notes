---
aliases:
  - policy
  - MDP policy
tags:
  - ms
---
# MDP policy
> [!info] Intro: 
> What actions to take and with what probability to take them. We can use different strategies for this.
> Defined as:
> $$
> \pi_{t}(a|s)
> $$
> **where:**
> This means probability of taking action a given that the agent is in state s. And $S_{t} = s$
> For deterministic policies:
> $$
> \pi_{t}(s) = action
> $$

1. **Greedy action selection ->** Always take the greedy action (the best one, greater rewards)
2. $\varepsilon$**-greedy ->** Select the greedy action with probability 1-$\varepsilon$ and explore with probability $\varepsilon$. 
3. **Preference based** -> Select action on rewards obtained. Develop preferences affecting the probability to take each action.

***
### Up
- [[1761231645-markovdecisionprocess|markov decision process]]
### Down
- [[1761239003-returnvsrewardinreinforcementlearning|return vs reward in reinforcement learning]]
***
