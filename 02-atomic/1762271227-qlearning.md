---
aliases:
- Q-Learning
tags:
- ms
---
# Q-Learning
> [!info] Intro: 
> Do not use the policy to choose the next action, but rather **take the best action.** A [[1762205273-temporaldifferencelearning|temporal difference learning]] implementation that is **off-policy** because of this property.


```pseudocode
Parameters: step size alpha in [0,1], small epsilon > 0. 
Init Q(s,a) for all states and all actions A(s) arbitrarily except that the Q(terminal,*) = 0. 
Loop for each episode:
	Init S
	Loop for each step of episode:
		Chose A from S using policy derived from Q
		Take action A, observe R,S' 
		Q(S,A) <- Q(S,A) + alpha[R + gamma max(a) Q(S',a) - Q(S,A)] -- Take the greedy action
		S <- S';
	until S is terminal
```

***
### Up
### Down
***