---
aliases:
- SARSA
tags:
- ms
---
# SARSA
> [!info] Intro: 
> SARSA, named after $S_{t},A_{t},R_{t+1},S_{t+1},A_{t+1}$. Is an implementation of reinforced learning that works **on policy**. Meaning that we use the policy to find the next action $A'$ to take. 

```pseudocode
Parameters: step size alpha in [0,1], small epsilon > 0. 
Init Q(s,a) for all states and all actions A(s) arbitrarily except that the Q(terminal,*) = 0. 
Loop for each episode:
	Init S
	Choose A from S using policy derived from Q(e.g. greedy or e-greedy)
	Loop for each step of episode:
		Take action A, observe R,S' 
		Choose A' from S' using policy derived from Q
		Q(S,A) <- Q(S,A) + alpha[R + gamma Q(S',A') - Q(S,A)]
		S <- S'; A <- A';
	until S is terminal
```

***
### Up
- [[1762205273-temporaldifferencelearning|TDL]]
### Down
***